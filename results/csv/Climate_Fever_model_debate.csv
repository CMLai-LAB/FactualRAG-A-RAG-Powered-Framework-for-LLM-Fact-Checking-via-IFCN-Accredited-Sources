model,result,accuracy,macro_precision,macro_recall,macro_f1,micro_precision,micro_recall,micro_f1,weighted_precision,weighted_recall,weighted_f1,avg_time_taken
llama3.1,result1,51.12,38.78,46.379999999999995,39.23,51.41,51.12,51.27,43.45,51.12,43.62,61.27
llama3.1,result2,50.56,39.06,45.86,39.06,50.849999999999994,50.56,50.7,43.78,50.56,43.44,66.47
llama3.1,result3,51.12,39.93,46.42,39.489999999999995,52.0,51.12,51.559999999999995,44.75,51.12,43.9,66.29
mistral,result1,43.82,42.53,40.36,35.52,44.07,43.82,43.94,46.17,43.82,38.66,57.13
mistral,result2,43.82,38.940000000000005,39.92,33.89,43.82,43.82,43.82,43.72,43.82,37.61,47.96
mistral,result3,44.379999999999995,43.75,40.849999999999994,36.05,44.379999999999995,44.379999999999995,44.379999999999995,47.47,44.379999999999995,39.269999999999996,52.55
gemma2,result1,44.940000000000005,37.89,40.9,34.58,44.940000000000005,44.940000000000005,44.940000000000005,42.52,44.940000000000005,38.4,83.61
gemma2,result2,47.19,41.28,42.89,37.059999999999995,47.73,47.19,47.46,46.379999999999995,47.19,41.21,93.04
gemma2,result3,46.07,45.32,42.15,36.620000000000005,47.13,46.07,46.589999999999996,48.96,46.07,40.23,95.59
phi4,result1,47.75,43.419999999999995,43.49,39.34,48.3,47.75,48.02,47.79,47.75,43.419999999999995,110.6
phi4,result2,48.88,46.63,44.800000000000004,40.17,49.15,48.88,49.01,50.0,48.88,43.87,116.8
phi4,result3,49.44,46.17,45.29,40.94,50.0,49.44,49.72,49.86,49.44,44.75,117.52
deepseek-r1:8b,result1,48.88,44.06,44.6,39.01,49.71,48.88,49.29,47.449999999999996,48.88,42.92,136.14
deepseek-r1:8b,result2,47.75,48.11,43.79,39.129999999999995,49.13,47.75,48.43,50.03999999999999,47.75,42.6,145.9
deepseek-r1:8b,result3,48.309999999999995,37.16,43.78,37.76,48.59,48.309999999999995,48.449999999999996,41.63,48.309999999999995,42.01,145.49
